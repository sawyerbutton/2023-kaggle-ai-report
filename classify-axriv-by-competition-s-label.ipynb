{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/tomsawyyer/classify-axriv-by-competition-s-label?scriptVersionId=130941824\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport json\nfrom datetime import datetime\nimport plotly.express as px\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-25T06:27:34.116994Z","iopub.execute_input":"2023-05-25T06:27:34.117367Z","iopub.status.idle":"2023-05-25T06:27:34.126037Z","shell.execute_reply.started":"2023-05-25T06:27:34.117339Z","shell.execute_reply":"2023-05-25T06:27:34.124994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Notebook Overview\n\nIn this notebook, we will attempt to analyze the source files from arXiv and use LLM and Transformer-based models to classify potential target papers.\n","metadata":{}},{"cell_type":"markdown","source":"## Reveiew Arxiv Metadata and filter","metadata":{}},{"cell_type":"code","source":"# Inspired by https://www.kaggle.com/code/leonidkulyk/kaggle-ai-report-topic-selection?scriptVersionId=129336498&cellId=24\n# import JSON data\ndict_arxiv = []\nfor line in open(\"/kaggle/input/2023-kaggle-ai-report/arxiv_metadata_20230510.json\", 'r'):\n    dict_arxiv.append(json.loads(line))","metadata":{"execution":{"iopub.status.busy":"2023-05-25T06:27:36.444724Z","iopub.execute_input":"2023-05-25T06:27:36.445357Z","iopub.status.idle":"2023-05-25T06:29:24.521616Z","shell.execute_reply.started":"2023-05-25T06:27:36.445325Z","shell.execute_reply":"2023-05-25T06:29:24.520521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(dict_arxiv) # total amount of papers","metadata":{"execution":{"iopub.status.busy":"2023-05-25T06:29:24.5238Z","iopub.execute_input":"2023-05-25T06:29:24.524152Z","iopub.status.idle":"2023-05-25T06:29:24.533388Z","shell.execute_reply.started":"2023-05-25T06:29:24.52412Z","shell.execute_reply":"2023-05-25T06:29:24.532046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Filter paper","metadata":{}},{"cell_type":"code","source":"# The number of scholarly articles is simply gigantic, so let's first filter out all the articles that were published before 2021\ndate_format = \"%a, %d %b %Y %H:%M:%S %Z\"\nlimit_year = datetime.strptime(\"2021\", \"%Y\")\n\nfiltered_arxiv = []\nfor article_meta in dict_arxiv:\n    parsed_date = datetime.strptime(\n        article_meta[\"versions\"][0][\"created\"], \n        date_format\n    )\n    if parsed_date >= limit_year:\n        filtered_arxiv.append(article_meta)","metadata":{"execution":{"iopub.status.busy":"2023-05-25T06:29:24.535009Z","iopub.execute_input":"2023-05-25T06:29:24.535657Z","iopub.status.idle":"2023-05-25T06:30:10.731099Z","shell.execute_reply.started":"2023-05-25T06:29:24.535622Z","shell.execute_reply":"2023-05-25T06:30:10.730092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(filtered_arxiv)","metadata":{"execution":{"iopub.status.busy":"2023-05-25T06:30:10.733747Z","iopub.execute_input":"2023-05-25T06:30:10.734244Z","iopub.status.idle":"2023-05-25T06:30:10.741704Z","shell.execute_reply.started":"2023-05-25T06:30:10.734205Z","shell.execute_reply":"2023-05-25T06:30:10.740778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filtered_arxiv[0] \n# one paper may have serveral categories\n# pay attention if you have analysis based on category","metadata":{"execution":{"iopub.status.busy":"2023-05-25T06:30:10.742887Z","iopub.execute_input":"2023-05-25T06:30:10.743644Z","iopub.status.idle":"2023-05-25T06:30:10.783133Z","shell.execute_reply.started":"2023-05-25T06:30:10.74361Z","shell.execute_reply":"2023-05-25T06:30:10.782263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Find all CS categories\n\n- Find all categories from https://arxiv.org/category_taxonomy","metadata":{}},{"cell_type":"code","source":"# List all CS related topics in arXiv Category Taxonomy.\ncategories = ['cs.AI', 'cs.AR', 'cs.CC', 'cs.CE', 'cs.CG', 'cs.CL', 'cs.CR', 'cs.CV', 'cs.CY', 'cs.DB', 'cs.DC', 'cs.DL', 'cs.DM', 'cs.DS', 'cs.ET', 'cs.FL', 'cs.GL', 'cs.GR', 'cs.GT', 'cs.HC', 'cs.IR', 'cs.IT', 'cs.LG', 'cs.LO', 'cs.MA', 'cs.MM', 'cs.MS', 'cs.NA', 'cs.NE', 'cs.NI', 'cs.OH', 'cs.OS', 'cs.PF', 'cs.PL', 'cs.RO', 'cs.SC', 'cs.SD', 'cs.SE', 'cs.SI', 'cs.SY']","metadata":{"execution":{"iopub.status.busy":"2023-05-25T06:30:10.784601Z","iopub.execute_input":"2023-05-25T06:30:10.784978Z","iopub.status.idle":"2023-05-25T06:30:10.791945Z","shell.execute_reply.started":"2023-05-25T06:30:10.784948Z","shell.execute_reply":"2023-05-25T06:30:10.790865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- If you want to get other topics from arXiv Category Taxonomy,\n- Try following code snippet(javascript) in Chrome Devtool -> Source Code -> Code Snippets\n\n```javascript\n// init array\nlet csContents = [];\n\n// get all h4 tags in page https://arxiv.org/category_taxonomy\nlet h4Tags = document.getElementsByTagName(\"h4\");\n\n// iterate array to get useful content\nfor(let i = 0; i < h4Tags.length; i++) {\n    if(h4Tags[i].textContent.startsWith(\"cs.\")) { // modify cs to your topic\n        let csContent = h4Tags[i].textContent.split(\" (\")[0];\n        csContents.push(csContent);\n    }\n}\n\nconsole.log(csContents);\n```","metadata":{}},{"cell_type":"markdown","source":"### Category filtered papers","metadata":{}},{"cell_type":"code","source":"categories_ids = {}\nfor category in categories:\n    category_ids = []\n    for article_meta in filtered_arxiv:\n        if category in article_meta[\"categories\"]:\n            category_ids.append(article_meta[\"id\"])\n    categories_ids[category] = set(category_ids)","metadata":{"execution":{"iopub.status.busy":"2023-05-25T06:30:10.793535Z","iopub.execute_input":"2023-05-25T06:30:10.794617Z","iopub.status.idle":"2023-05-25T06:30:21.23217Z","shell.execute_reply.started":"2023-05-25T06:30:10.794564Z","shell.execute_reply":"2023-05-25T06:30:21.231119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for key, value_set in categories_ids.items():\n    print(f\"{key} has {len(value_set)} items in its set.\")","metadata":{"execution":{"iopub.status.busy":"2023-05-25T06:30:21.233418Z","iopub.execute_input":"2023-05-25T06:30:21.233805Z","iopub.status.idle":"2023-05-25T06:30:21.241024Z","shell.execute_reply.started":"2023-05-25T06:30:21.233773Z","shell.execute_reply":"2023-05-25T06:30:21.240008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.bar(x=categories_ids.keys(), y=[len(x) for x in categories_ids.values()], text_auto='.2s')\nfig.update_layout(\n    xaxis_title=\"Category\",\n    yaxis_title=\"Articles count\"\n)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-25T06:30:21.242286Z","iopub.execute_input":"2023-05-25T06:30:21.242828Z","iopub.status.idle":"2023-05-25T06:30:23.582521Z","shell.execute_reply.started":"2023-05-25T06:30:21.242789Z","shell.execute_reply":"2023-05-25T06:30:23.581641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Thanks for https://www.kaggle.com/code/leonidkulyk/kaggle-ai-report-topic-selection/notebook#--1.-Topics-selection\n- I've just realized that I can utilize the ChatGPT API & lang-chain to categorize these papers into the following topics.\n- The current Kaggle competition's topic classification differs from the classification method used in arXiv. \n- If someone plans to generate papers using generative AI in the future, it's advisable to first classify the papers based on the Kaggle's topics. The topics for the competition are as follows:\n\n\n1. Text data\n2. Image and/or video data\n3. Tabular and/or time series data\n4. Kaggle Competitions\n5. Generative AI\n6. AI ethics\n7. Other\n\n","metadata":{}},{"cell_type":"markdown","source":"### Prepare content which will be classify by llm","metadata":{}},{"cell_type":"code","source":"\n# we will use paper's title + abstract content\n# Because our mission is about AI report, so I will drop all papars without 'cs' category to save my API credits\n\nextracted_data = [{\"id\": item[\"id\"], \"title\": item[\"title\"], \"abstract\": item[\"abstract\"]} for item in filtered_arxiv if 'cs' in item[\"categories\"]]\n\ndf = pd.DataFrame(extracted_data)\n\ndf['title_abstract'] = df['title'] + ' ' + df['abstract']\n\ndf = df.drop(['title', 'abstract'], axis=1)\n\nprint(df) # total 231386 LOL my credit","metadata":{"execution":{"iopub.status.busy":"2023-05-25T06:30:23.586517Z","iopub.execute_input":"2023-05-25T06:30:23.587223Z","iopub.status.idle":"2023-05-25T06:30:24.718179Z","shell.execute_reply.started":"2023-05-25T06:30:23.587197Z","shell.execute_reply":"2023-05-25T06:30:24.716818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.loc[0]['title_abstract']) \n# for demo usage\ntemp_title_abstract = df.loc[0]['title_abstract']","metadata":{"execution":{"iopub.status.busy":"2023-05-25T06:30:24.720124Z","iopub.execute_input":"2023-05-25T06:30:24.720533Z","iopub.status.idle":"2023-05-25T06:30:24.777447Z","shell.execute_reply.started":"2023-05-25T06:30:24.720497Z","shell.execute_reply":"2023-05-25T06:30:24.776486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prepare candidate labels\ncandidate_labels = [\n    \"Text data\", \n    \"Image and/or video data\", \n    \"Tabular and/or time series data\", \n    \"Generative AI\", \n    \"AI ethics\",\n    \"Other\"\n]","metadata":{"execution":{"iopub.status.busy":"2023-05-25T06:30:24.778891Z","iopub.execute_input":"2023-05-25T06:30:24.779265Z","iopub.status.idle":"2023-05-25T06:30:24.784162Z","shell.execute_reply.started":"2023-05-25T06:30:24.779232Z","shell.execute_reply":"2023-05-25T06:30:24.78318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using LangChain\n\n- We now have our dataset and labels at hand.\n- It's time to proceed with classification.\n- we'll utilize the ChatGPT model, gpt3.5-turbo, and lang chain for easy use of the prompt template.\n- If you're new to lang chain, I recommend going through this.https://python.langchain.com/en/latest/modules/prompts/prompt_templates.html\n- Given that we have 220k papers on CS topics, this would result in substantial API credit usage.\n- Therefore, I've decided to adopt a zero-shot approach to create a smaller dataset for the Bert-downstream task(label 50 papers) for first attempt","metadata":{}},{"cell_type":"code","source":"# intall deps\n# Todo\n\n!pip install langchain\n!pip install openai\n!pip install tqdm","metadata":{"execution":{"iopub.status.busy":"2023-05-25T06:30:24.785806Z","iopub.execute_input":"2023-05-25T06:30:24.786311Z","iopub.status.idle":"2023-05-25T06:31:06.090877Z","shell.execute_reply.started":"2023-05-25T06:30:24.78628Z","shell.execute_reply":"2023-05-25T06:31:06.089688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define a progress function for API-Calliing, killing time\nfrom tqdm import tqdm\nimport time\n\ndef show_progress(iterable):\n    with tqdm(iterable) as pbar:\n        for item in pbar:\n            pbar.set_description(f\"Progress: {pbar.n + 1}/50\")","metadata":{"execution":{"iopub.status.busy":"2023-05-25T06:31:06.094218Z","iopub.execute_input":"2023-05-25T06:31:06.09464Z","iopub.status.idle":"2023-05-25T06:31:06.100536Z","shell.execute_reply.started":"2023-05-25T06:31:06.094582Z","shell.execute_reply":"2023-05-25T06:31:06.099665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain.llms import OpenAI\n# llm = OpenAI(openai_api_key=\"OPENAI_API_KEY\")\nllm = OpenAI(openai_api_key=\"\", model_name=\"gpt-3.5-turbo\",temperature=0.0)\n\nfrom langchain import PromptTemplate,LLMChain\n\n\ntemplate = \"\"\"\nConsider the following categories: 'Text data', 'Image and/or video data', 'Tabular and/or time series data', 'Generative AI', 'AI ethics', and 'Other'. Given a piece of text,\nplease strictly classify it into one of these categories exactly as written. \nDo not add any extra words, punctuation, or provide paraphrased versions of the categories. \nThe input text is: {titleAbstract}\n\"\"\"\n\nprompt = PromptTemplate(\n    input_variables=[\"titleAbstract\"],\n    template=template,\n)\n\n# try template result to validate your prompt\nprompt.format(titleAbstract=temp_title_abstract)\n\nllm_chain = LLMChain(prompt=prompt, llm=llm)\nllm_chain.run(temp_title_abstract)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-25T06:32:53.557959Z","iopub.execute_input":"2023-05-25T06:32:53.558355Z","iopub.status.idle":"2023-05-25T06:32:53.691499Z","shell.execute_reply.started":"2023-05-25T06:32:53.558327Z","shell.execute_reply":"2023-05-25T06:32:53.690092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prepare label paper, to save my credit, 50 parpers are considered.\nimport string\ndf_small = df.iloc[:50]\ndf_small['api-label'] = None\n\ndef get_gpt_label():\n    for i in range(len(df_small)):\n        text = df_small['title_abstract'].iloc[i]\n        label = llm_chain.run(text)\n        # Uncomment the following print function if you wish to review the result.\n        # Be prepared for extensive and potentially tedious output.\n        # print(\"original content is {} label predicted by API is {}\".format(text,label))\n        # print()\n        df_small['api-label'].iloc[i] = label.strip(string.punctuation) # Hard coding is necessary because the API response does not strictly follow my request\n        yield i\n\n        \nshow_progress(get_gpt_label())","metadata":{"execution":{"iopub.status.busy":"2023-05-25T06:31:09.061928Z","iopub.status.idle":"2023-05-25T06:31:09.064956Z","shell.execute_reply.started":"2023-05-25T06:31:09.064713Z","shell.execute_reply":"2023-05-25T06:31:09.064737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_small.head(20)","metadata":{"execution":{"iopub.status.busy":"2023-05-25T06:31:09.068094Z","iopub.status.idle":"2023-05-25T06:31:09.069877Z","shell.execute_reply.started":"2023-05-25T06:31:09.069637Z","shell.execute_reply":"2023-05-25T06:31:09.06966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_counts = df_small['api-label'].value_counts().reset_index()\n\nlabel_counts.columns = ['api-label', 'count']\n\nfig = px.bar(label_counts, x='api-label', y='count', title='Distribution of API Labels')\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-25T06:31:09.074669Z","iopub.status.idle":"2023-05-25T06:31:09.075452Z","shell.execute_reply.started":"2023-05-25T06:31:09.075215Z","shell.execute_reply":"2023-05-25T06:31:09.075237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using zero shot classification model from HuggingFace\n\n- The results are not entirely convincing as the tabular and time series data have excessive counts.\n- I intend to experiment with other models for comparison and to possibly improve the results.\n- This time we use Bart-Large-mnli model from Facebook\n- We both use first 50 parper to classify, let's see the difference","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-mnli\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"facebook/bart-large-mnli\")\n\nnlp = pipeline(\"zero-shot-classification\", model=model, tokenizer=tokenizer)\n\ntext = \"The research paper discusses the application of deep learning for natural language processing.\"\n\nresult = nlp(text, candidate_labels)\n\nmax_score_index = result['scores'].index(max(result['scores']))\npredicted_label = result['labels'][max_score_index]\nprint(predicted_label) # validate model work","metadata":{"execution":{"iopub.status.busy":"2023-05-25T06:33:07.284998Z","iopub.execute_input":"2023-05-25T06:33:07.285407Z","iopub.status.idle":"2023-05-25T06:33:39.730303Z","shell.execute_reply.started":"2023-05-25T06:33:07.285375Z","shell.execute_reply":"2023-05-25T06:33:39.726912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_small_bart = df.iloc[:50]\ndf_small_bart['api-label'] = None\n\ndef get_bart_label():\n    for i in range(len(df_small_bart)):\n        text = df_small_bart['title_abstract'].iloc[i]\n        label = nlp(text, candidate_labels)\n        max_score_index = result['scores'].index(max(result['scores']))\n        predicted_label = result['labels'][max_score_index]\n        df_small_bart['api-label'].iloc[i] = predicted_label.strip(string.punctuation)\n        yield i\nshow_progress(get_bart_label())","metadata":{"execution":{"iopub.status.busy":"2023-05-25T06:31:09.085121Z","iopub.status.idle":"2023-05-25T06:31:09.08589Z","shell.execute_reply.started":"2023-05-25T06:31:09.085652Z","shell.execute_reply":"2023-05-25T06:31:09.085675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_small_bart.head(20) # What a ridiculous result! All papers has been classify to Other topic.","metadata":{"execution":{"iopub.status.busy":"2023-05-25T06:31:09.08723Z","iopub.status.idle":"2023-05-25T06:31:09.087943Z","shell.execute_reply.started":"2023-05-25T06:31:09.08772Z","shell.execute_reply":"2023-05-25T06:31:09.087742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Considering the star count associated with the `Zero-Shot Classification` task on Huggingface,\n- There seems to be no reason to choose any model other than 'facebook/bart-large-mnli'.\n- Feel free to review or select your choice through this link: https://huggingface.co/models?pipeline_tag=zero-shot-classification&library=transformers&sort=downloads.\n","metadata":{}},{"cell_type":"markdown","source":"# We are not Finished yet\n\nAt this moment, I find myself at a crossroads with two divergent paths in front of me:\n\n1. **Option One:** Classify all the papers using the API call, but this will consume a large amount of credits. Based on the current consumption rate, classifying 50 papers using the gpt3.5-turbo model costs roughly `0.4 dollars`. Therefore, classifying `230,000` pieces of data would cost approximately `1800 dollars`. While this might not be a large sum for a lab or a company, it is certainly a high price for an individual like me and beyond what I can afford.\n\n2. **Option Two:** Use all my credit allowance for this month to classify about 700 data pieces, creating a small training set. I could then use this training set to distill or fine-tune a Bert or similar encoder model, and subsequently classify the entire set of 230,000 papers. However, I am concerned that 700 pieces of data are too few to effectively fine-tune the parameters.\n\n### Now, I am stuck in this predicament. If you have any good suggestions, please discuss them in the comment section.\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}